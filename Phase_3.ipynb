{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52069286-13fe-4542-ab00-e0d7bb8a7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for a dataframe which will be used to store the book information prior to writing to a csv\n",
    "import pandas as pd\n",
    "\n",
    "# Import the requests package for sending HTTP requests\n",
    "import requests\n",
    "\n",
    "# Import the beautifulsoup package for parsing the website's HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import re for RegEx build which is used to extract the availability of a book\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8de70d2-0f1b-4f8c-aacb-7d97846a8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function creates a list of book categories by passing the url of the site and returning a list of categories\n",
    "def get_book_categories(site_url):\n",
    "    # Get the site's HTML\n",
    "    page = requests.get(site_url)\n",
    "\n",
    "    # Create soup object for parsing\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Look for the side_categories container which contains a list of categories on the site\n",
    "    category_container = soup.find(\"div\", class_=\"side_categories\")\n",
    "\n",
    "    # Create a list to store all the extracted categories by a key-value pair with the get being the name of the book and the value being the book's url\n",
    "    category_list = {}\n",
    "\n",
    "    # Loop through each 'a' tag found in the category_container\n",
    "    for category in category_container.find_all(\"a\", href=True):\n",
    "        # Define the key as the name of the category\n",
    "        key = category.text.strip()\n",
    "\n",
    "        # Define the value as the URL of the category\n",
    "        value = site_url+category[\"href\"]\n",
    "\n",
    "        # Write the key-value pair to the category_list\n",
    "        category_list[key] = value\n",
    "\n",
    "    # Remove the first item in the list since it is a header and not a category\n",
    "    category_list.pop(list(category_list.keys())[0])\n",
    "    \n",
    "    return category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e948654-ffb0-4542-ab3c-321ce7dae628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function creates a list of all books in a category by passing the URL of the category and returning a list of books in the category\n",
    "def get_books_in_category(category_url):\n",
    "    # Create a list to store the extracted book URLs\n",
    "    books_in_category_url = []\n",
    "\n",
    "    # Split the category's URL so the index can be changed for each page in the category\n",
    "    base_url = category_url.split(\"index.html\")[0]\n",
    "\n",
    "    # Define the first page's index\n",
    "    current_page_index = \"index.html\"\n",
    "\n",
    "    # Check to see if the category has pagination, if it does loop through each page\n",
    "    while True:\n",
    "        # Define the URL of the page in the category by concaenating the base URL and the page index\n",
    "        current_page_url = base_url+current_page_index\n",
    "\n",
    "        # Get the category page's HTML\n",
    "        page = requests.get(current_page_url)\n",
    "\n",
    "        # Create soup object for parsing\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # Look for the table on the page whcih contains the books\n",
    "        books_container = soup.find_all(\"h3\")\n",
    "\n",
    "        # Get the URL of the books on the current page by looping through each item in the books_container\n",
    "        for image_container in books_container:\n",
    "            # Define the URL by finding the 'a' tag\n",
    "            book_href = image_container.find(\"a\", href=True)\n",
    "\n",
    "            # Create the final book URL by appending the base URL and the HREF of the book\n",
    "            # Since the HREF of the book is a relative URL, replace the relative URL with an absolute URL\n",
    "            books_in_category_url.append(book_href[\"href\"].replace(\"../../../\",\"https://books.toscrape.com/catalogue/\"))    \n",
    "\n",
    "        # Check to see if there is another page for the category by looking for the 'next' list item\n",
    "        next_page = (soup.find(\"li\", class_=\"next\"))\n",
    "\n",
    "        # If there is a next page, get the url of the page at set that as the curren_page_index so when the function loops it navigates to the next page\n",
    "        if next_page:\n",
    "            current_page_index = next_page.find(\"a\")[\"href\"]\n",
    "        # If there is not another page to extract data from, exit the loop\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return books_in_category_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2acfc08-320d-4f83-9d82-b1ad0101dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function scrapes data for a single book by passing the book's URL and the book's elements are returned\n",
    "def scrape_book_data(book_page_url):\n",
    "    # Get the book's page HTML\n",
    "    page = requests.get(book_page_url)\n",
    "    \n",
    "    # Create soup object for parsing\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Get the Product Description by finding the product description container\n",
    "    description_container = soup.find(\"div\", id=\"product_description\")\n",
    "    \n",
    "    # As some books do not contain a description on the page, check to see if the container exists\n",
    "    # If the container does exist get the description text\n",
    "    if description_container:\n",
    "        # Find the next 'p' tag after the descrption container\n",
    "        product_description =  description_container.find_next(\"p\").text.strip()\n",
    "    else:\n",
    "        # Return nothing if the description container is not found on the page\n",
    "        product_description = \"\"\n",
    "\n",
    "    # Get book category by finding the breadcrumb\n",
    "    breadcrumb_container = soup.find(\"ul\", class_=\"breadcrumb\")\n",
    "\n",
    "    # Extract all of the 'a' tags from the breadcrumb\n",
    "    list_item = breadcrumb_container.find_all(\"a\")\n",
    "\n",
    "    # Get the category as the 3rd entry in the list\n",
    "    category = list_item[2].text.strip()\n",
    "\n",
    "    # Get the book title from the list item's in the breadcrumb\n",
    "    book_title = breadcrumb_container.find(\"li\",class_=\"active\").text\n",
    "\n",
    "    # Find the container that holds the book's rating\n",
    "    rating_container = soup.find(\"div\",class_=\"col-sm-6 product_main\")\n",
    "\n",
    "    # Get the rating from the rating container by extracting the class for star-rating\n",
    "    # The class extracts a list where the first item is the class and the second item is the actual rating\n",
    "    review_rating = rating_container.find(\"p\", class_=\"star-rating\").get(\"class\")[1]\n",
    "\n",
    "    # Get the Product Information table data which contains multiple elements for the book\n",
    "    product_information_container = soup.find(\"table\", class_=\"table table-striped\")\n",
    "\n",
    "    # Since multiple data points need to be extracted from the table, create a dictionary to store all the data points in\n",
    "    product_information = {}\n",
    "\n",
    "    # loop the table rows in the product information container\n",
    "    for table_row in product_information_container.find_all(\"tr\"):\n",
    "        # Define the key as the extracted table header (TH) which is the name of the element\n",
    "        key = table_row.find(\"th\").text.strip()\n",
    "        \n",
    "        # Define the value as the extracted table description (TD) whichs is the value of the element\n",
    "        value = table_row.find(\"td\").text.strip()\n",
    "\n",
    "        # Write the key:value pair to the dictionary\n",
    "        product_information[key] = value\n",
    "\n",
    "    # Define the variables and gather the required information\n",
    "    universal_product_code = product_information[\"UPC\"]\n",
    "    price_including_tax = product_information[\"Price (incl. tax)\"]\n",
    "    price_excluding_tax = product_information[\"Price (excl. tax)\"]\n",
    "\n",
    "    # Get the quantity available. The extracted value comes out as 'In stock (XX availabile)\". Extract the actual number via RegEx\n",
    "    quantity_available = re.findall(r\"\\d+\",product_information[\"Availability\"])[0]\n",
    "    \n",
    "    # Get the Image URL by finding the thumbnail container\n",
    "    thumbnail_container = soup.find(\"div\",class_=\"thumbnail\")\n",
    "\n",
    "    # Create the URL for the image. Since the image URL is relative, make it absolute\n",
    "    image_url = thumbnail_container.find(\"img\")[\"src\"].replace(\"../..\",\"https://books.toscrape.com\")\n",
    "\n",
    "    # Write the extracted values to the book_extracted_information list to be returned\n",
    "    book_extracted_information = [book_page_url,universal_product_code,book_title,price_including_tax,price_excluding_tax,quantity_available,product_description,category,review_rating,image_url]\n",
    "    \n",
    "    return book_extracted_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a8cb90-a033-44c4-b537-149dc8ea95cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Create pandas dataframe and set the column headers to the data elements being extracted\n",
    "book_data = pd.DataFrame(columns=[\"product_page_url\",\"universal_product_code\",\"book_title\",\"price_including_tax\",\"price_excluding_tax\",\"quantity_available\",\"product_description\",\"category\",\"review_rating\",\"image_url\"])\n",
    "\n",
    "# Define the URL to scrape data off of and define it as the url variable\n",
    "site_page_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Gather list of categories on the site by calling the function\n",
    "scraped_category_list = get_book_categories(site_page_url)\n",
    "\n",
    "# Gather list of books in a category by calling function and passing the category url\n",
    "# Start by looping through each category gathered\n",
    "for category, category_url in scraped_category_list.items():\n",
    "    # For each category call the function that creates the list of books in the category\n",
    "    books_in_category_list = get_books_in_category(category_url)\n",
    "\n",
    "    # Loop the list of books in the category\n",
    "    for book_url in books_in_category_list:\n",
    "        # For each book, call the function which extracts the data for the book\n",
    "        scraped_book_data = scrape_book_data(book_url)\n",
    "\n",
    "        # Check to see if the extracted book is already in the list of scraped book data, if it is not, add it to the dataframe\n",
    "        if not book_data[\"universal_product_code\"].isin([scraped_book_data[1]]).any():\n",
    "            book_data.loc[len(book_data)] = scraped_book_data\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e23543-46ec-4e4f-9c60-3a598806d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the book_data dataframe to CSV\n",
    "book_data.to_csv(\"Phase_III_Extracted_Book_Data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
